# -*- coding: utf-8 -*-
"""Sentiment Analaysis(South Korea).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S7IQyp_ZmB54czhvwETHIdvCti2GHPAM
"""
import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from textblob import TextBlob
from wordcloud import WordCloud
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from imblearn.over_sampling import SMOTE
import streamlit as st
import warnings
warnings.filterwarnings('ignore')



# Download required NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')

# Step 1: Web scrape Wikipedia page for Malaysia
url = "https://en.wikipedia.org/wiki/South_Korea"
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')

print(f"Response Status: {response.status_code}")
print(f"Page Title: {soup.title.string}")

# Step 2: Scrape only text
paragraphs = []
content_div = soup.find("div", {"id": "mw-content-text"})
if content_div:
    for p in content_div.find_all("p"):
        if p.text.strip():
            paragraphs.append(p.text)

# Combine all paragraphs into one text
raw_text = ' '.join(paragraphs)

print(f"Total characters extracted: {len(raw_text)}")
print(f"Preview of the text: {raw_text[:500]}...")

# Step 3: Clean Text and preprocessing
def clean_text(text):
    # Remove citations [1], [2], etc.
    text = re.sub(r'\[\d+\]', '', text)
    # Remove special characters and digits
    text = re.sub(r'[^\w\s.]', '', text)
    # Replace multiple spaces with a single space
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

cleaned_text = clean_text(raw_text)

print(f"Total characters after cleaning: {len(cleaned_text)}")
print(f"Preview of the cleaned text: {cleaned_text[:500]}...")

# Step 4: Use Sentence Tokenizer and sentence tokenize
sentences = sent_tokenize(cleaned_text)
print(f"Total number of sentences: {len(sentences)}")
print(f"First 5 sentences:")
for i, sent in enumerate(sentences[:5]):
    print(f"{i+1}. {sent}")

# Step 5: Use TextBlob and analyze_sentiment() function to calculate sentiment of sentences
def get_sentiment(sentence):
    analysis = TextBlob(sentence)
    polarity = analysis.sentiment.polarity

    if polarity > 0.1:
        return 'positive'
    elif polarity < -0.1:
        return 'negative'
    else:
        return 'neutral'

# Calculate sentiment for each sentence
sentiment_results = []
for sentence in sentences:
    blob = TextBlob(sentence)
    polarity = blob.sentiment.polarity
    subjectivity = blob.sentiment.subjectivity
    sentiment = get_sentiment(sentence)

    sentiment_results.append({
        'sentence': sentence,
        'polarity': polarity,
        'subjectivity': subjectivity,
        'sentiment': sentiment
    })

# Step 6: Create a dataframe of sentences and sentiment
sentiment_df = pd.DataFrame(sentiment_results)

print(f"Dataframe shape: {sentiment_df.shape}")

print("\nFirst 5 rows of the sentiment dataframe:")

print(sentiment_df.head())

print("\nSentiment distribution:")
print(sentiment_df['sentiment'].value_counts())

# Visualize sentiment distribution
plt.figure(figsize=(8, 6))
sentiment_df['sentiment'].value_counts().plot(kind='bar', color=['gray', 'green', 'red'])
plt.title('Sentiment Distribution - Malaysia')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

# Step 7: Word tokenize the text data
words = word_tokenize(cleaned_text.lower())
print(f"Total words found: {len(words)}")
print(f"First 20 words: {words[:20]}")

# Step 8: Remove stopwords
stop_words = set(stopwords.words('english'))
filtered_words = [word for word in words if word.isalpha() and word not in stop_words]

print(f"Words after removing stopwords: {len(filtered_words)}")
print(f"First 20 filtered words: {filtered_words[:20]}")

# Generate word cloud
wordcloud = WordCloud(width=800, height=600, background_color='white').generate(' '.join(filtered_words))
plt.figure(figsize=(10, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud - Malaysia')
plt.show()

# Most common words
word_counts = Counter(filtered_words)
common_words = word_counts.most_common(20)
words_df = pd.DataFrame(common_words, columns=['word', 'count'])

plt.figure(figsize=(10, 6))
plt.barh(words_df['word'], words_df['count'])
plt.title('Top 20 Most Common Words - Malaysia')
plt.xlabel('Count')
plt.ylabel('Word')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# Step 11: Apply TFIDF Vectorizer on sentences and sentiment dataframe
# Step 12: Use only positive and negative sentiments only & ignore neutral
filtered_sentiment_df = sentiment_df[sentiment_df['sentiment'] != 'neutral']
print(f"Dataframe shape after removing neutral sentiments: {filtered_sentiment_df.shape}")
print(filtered_sentiment_df['sentiment'].value_counts())

# Prepare data for ML models
X = filtered_sentiment_df['sentence']
y = filtered_sentiment_df['sentiment']

# Convert text to TFIDF features
tfidf_vectorizer = TfidfVectorizer(max_features=5000, min_df=2)
X_tfidf = tfidf_vectorizer.fit_transform(X)

print(f"TFIDF matrix shape: {X_tfidf.shape}")

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42, stratify=y)

print(f"Training samples: {X_train.shape[0]}")
print(f"Testing samples: {X_test.shape[0]}")

import seaborn as sns

# Logistic Regression
lr_model = LogisticRegression(max_iter=200)
lr_model.fit(X_train, y_train)

# Predictions
lr_preds = lr_model.predict(X_test)

# Evaluation
print("Logistic Regression Classification Report:")
print(classification_report(y_test, lr_preds))

# Confusion Matrix
cm_lr = confusion_matrix(y_test, lr_preds)
sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', xticklabels=lr_model.classes_, yticklabels=lr_model.classes_)
plt.title("Confusion Matrix - Logistic Regression")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Multinomial Naive Bayes
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)

# Predictions
nb_preds = nb_model.predict(X_test)

# Evaluation
print("Naive Bayes Classification Report:")
print(classification_report(y_test, nb_preds))

# Confusion Matrix
cm_nb = confusion_matrix(y_test, nb_preds)
sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Oranges', xticklabels=nb_model.classes_, yticklabels=nb_model.classes_)
plt.title("Confusion Matrix - Naive Bayes")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

print("Accuracy Score:", accuracy_score(y_test, nb_preds))

